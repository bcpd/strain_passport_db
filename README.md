# Strain_Passport_DB

The repository contains scripts to generate an sqlite database and produce reports containing strain-associated information.
For this version of the passport, we used species-level MAGs identified in a collection of microbial-based products and a mock community for the Phase 1 proof-of-concept.



## Preliminary steps

To construct the database, MAGs need to be annotated with bakta and the predicted CDSs with DRAM. MAGs, their stats, and their annotation come from processing the samples with our pipeline.

*0. Project information*.
 We use a unique alpha numeric project code to save the information of the MAGs. MAGs are named by default as MAGXX, where XX is a number. Thus we need to add a prefix to their name to specify their origin and avoid mixing information from multiple organisms in the database.
The name of the project needs to be stores in a file called "Project_code.txt" in a single line. The project code needs to be alpha numeric (no spaces or symbols).

*1. Genome information*
We need to parse the genome annotation summary files which also include information related to the genome itself such as number genome size, N50, etc. 
We used the `parse_annotation_summaries.R` script for this.
- Input: Annotation tables from bakta (one .txt per MAG)
- Output: genome_summaries.tsv (stored in the Results folder of this repository)


*2. Merging of DRAM/bakta annotations tables.* 
Annotation tables are joined based on the GeneID, which is a combination of the MAG ID and the Locus tag (`merge_annotations.R`).
- Input: Annotation tables from bakta (one .tsv per MAG), genome summaries from bakta (one .txt file per MAG), and DRAM (one .tsv file for all the MAGs).
- Output: merged_annotations.tsv (stored in the Results folder of this repository)


*3. Expert systems.* Per-MAG json files (generated by bakta) are parsed into virulence factors and AMR genes, which are then mapped to virulence categories and AMR classes (`get_expert_systems.R`).
- Input: Folder with json files from processing genomes with bakta, Virulence factors description table (VFs.xls) and DNA sequences core set both from the Virulence factor database (VFDB_setA_nt.fas).
- Output: amrfinderplus annotations for all the mags (armfinder_annotation.csv), amrfinderplus mapping file(amrfinderplus_map.csv), vfdb annotations for all mags (vfdb-annotations.csv), vfdb mapping file (vfdb_map.csv). All these files are now stored in the Results folder of this repository.


*4. Patent search*
US patents records (2000-present) are searched and parsed for each species-level MAG (`patent_search_v2.R`).
- Input: Taxonomy table from the initial pipeline (gtdbtk.bac120.summary.tsv), project name.
- Output: For each MAG with species-level information, one table with patent search results (stored in the Results folder of this repository)


*5. Literature searches.* 
Pubmed records (2000-present) are searched and parsed for each species-level MAG (`pubmed_v4.R`).
- Input: Taxonomy table from the initial pipeline
- Output: For each MAG with species-level information, one table with Pubmed records (stored in the Results folder of this repository)

bioRxiv records (2000-present) are searched and parsed for each species-level MAG (`biorxiv_searcher_passport.R`). This requires downloading the whole databaset (~1 hour).
- Input: Taxonomy table from the initial pipeline
- Output: For each MAG with species-level information, one table with Medrxivr records (stored in the Results folder of this repository)


*6. BacDive API.*
To access the BacDive database, a log-in profile is required. The database is searched and parsed with the R-based API for each species-level MAG (`get_bacdive_data.r`).


*7. Median MAG coverage*
We process the median MAG coverage results to attach the project code to it and make the table long instead of wide (`process_MAG_coverage.R`)
- Input: median_coverage_genomes.tsv from the previous pipeline, project code.
- Output: Median coverage file with MAG names modified to have the project code (median_MAG_coverage.tsv, stored in the Results folder of this repository) 

## Strain passport construction
-	`webScraperScraper.py`. Pulls data from the tables into StrainPassport.db.
-	`createStrainPassports.R`. Creates the database connection in R and passes variables to the strain passport page template (RMD file). This script also obtains and parses KEGG xml files describing metabolic modules.
-	`StrainPassportTemplate.Rmd`. Strain passport page template. This is where db queries are called.
